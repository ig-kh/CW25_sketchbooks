{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fba528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm\n",
    "import polars as pl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d326dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_RQVAE(nn.Module):\n",
    "    def __init__(self, embedding_dim, latent_dim, codebook_size, quantization_depth=4, dnn_latent_dim=256):\n",
    "        super(nn.Module, self).__init__()\n",
    "\n",
    "        self.encoder_dnn = nn.Sequential(\n",
    "            [nn.Gelu(), \n",
    "             nn.Linear(dnn_latent_dim, latent_dim),\n",
    "             ]\n",
    "        )\n",
    "        self.decoder_dnn = nn.Sequential(\n",
    "            [\n",
    "             nn.Linear(latent_dim, dnn_latent_dim),\n",
    "             nn.Gelu(),\n",
    "             ]\n",
    "        )\n",
    "\n",
    "        self.encoder_adapter = nn.Linear(embedding_dim, dnn_latent_dim)\n",
    "        self.decoder_adapter = nn.Linear(dnn_latent_dim, embedding_dim)\n",
    "\n",
    "        self.codebooks = nn.ModuleList([nn.Embedding(codebook_size, latent_dim) for _ in range(quantization_depth)])\n",
    "        self.quantization_depth = quantization_depth  # Set to 4 for this case\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x: (B, T, E) - batch size, sequence length, embedding dimension\n",
    "        z = self.encoder_adapter(x)  # Latent vectors: (B, T, L)\n",
    "        z = self.encoder_dnn(z)\n",
    "        hat_z = torch.zeros_like(z)  # Initialize quantized latent vectors\n",
    "        indices_list = []  # Collect indices for each depth\n",
    "\n",
    "        # Residual quantization with four different codebooks\n",
    "        for d in range(self.quantization_depth):\n",
    "            residual = z - hat_z\n",
    "            # Compute distances to current codebook\n",
    "            distances = torch.sum((residual.unsqueeze(2) - self.codebooks[d].weight.unsqueeze(0).unsqueeze(0)) ** 2, dim=3)\n",
    "            # Get indices of nearest codebook entries\n",
    "            c_d = torch.argmin(distances, dim=2)  # (B, T)\n",
    "            # Get quantized vectors\n",
    "            q_d = self.codebooks[d](c_d)  # (B, T, L)\n",
    "            hat_z = hat_z + q_d  # Update approximation\n",
    "            indices_list.append(c_d)\n",
    "\n",
    "        # Stack indices to get four tokens per position: (B, T, 4)\n",
    "        indices = torch.stack(indices_list, dim=2)\n",
    "        # Apply straight-through estimator for gradient flow\n",
    "        hat_z = hat_z + (z - z.detach())\n",
    "        # Reconstruct input embeddings\n",
    "        x_hat = self.decode_dnn(hat_z)  # (B, T, E)\n",
    "        x_hat = self.decoder_adapter(x_hat)\n",
    "        return x_hat, hat_z, z, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb26bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rqvae_combined_loss(x, x_hat, hat_z, z, beta = 0.25):\n",
    "    recon_loss = F.mse_loss(x_hat, x)\n",
    "    commit_loss = F.mse_loss(z.detach(), hat_z)\n",
    "\n",
    "    return recon_loss + beta * commit_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b1499a",
   "metadata": {},
   "source": [
    "Горячий случай"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1342ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbeddingDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.row(idx, named=True)\n",
    "        return item['item_id'], torch.tensor(item['emb'], dtype=torch.float32).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "df = pl.read_parquet(\"./data/lvl2_data/items.parquet\")\n",
    "batch_size = 128\n",
    "dataset = TextEmbeddingDataset(df)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac5342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN_RQVAE(512, 256, 128, 4, 256)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552e4be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train loop\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "model.train()\n",
    "loss_hist = []\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        x = batch.to(device)  # Shape: (batch_size, 1, embedding_dim)\n",
    "        optimizer.zero_grad()\n",
    "        x_hat, hat_z, z, indices = model(x)\n",
    "        loss = rqvae_combined_loss(x, x_hat, hat_z, z)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    loss_hist.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f8762",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./rqvae_trained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b764fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
